# Text Embeddings and Vector Database with FAISS
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Initialize the SentenceTransformer model and FAISS index
embedder = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose other models based on your needs
dimension = 384  # Dimensionality of the embeddings generated by the model
index = faiss.IndexFlatL2(dimension)  # L2 distance-based FAISS index

def embed_and_store(texts):
    """
    Generates embeddings for a list of texts and stores them in a FAISS index.
    
    :param texts: A list of strings to be embedded and stored.
    """
    embeddings = embedder.encode(texts)  # Generate embeddings
    index.add(np.array(embeddings))  # Store embeddings in the FAISS index

def retrieve_context(query, top_k=5):
    """
    Retrieves the most relevant contexts from the FAISS index for a given query.
    
    :param query: The query text.
    :param top_k: The number of top contexts to retrieve.
    :return: Indices of the top-k most relevant contexts.
    """
    query_embedding = embedder.encode([query])  # Embed the query text
    distances, indices = index.search(np.array(query_embedding), k=top_k)  # Search for the closest embeddings
    return indices.flatten()  # Return the indices of the closest contexts

# Example usage
if __name__ == "__main__":
    embed_and_store(["Sample text for embedding"])
    indices = retrieve_context("Sample query")
    print(indices)
